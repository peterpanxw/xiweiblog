---
title: "The Road to Diffusion Models: Introduction"
date: 2024-07-09T20:57:54+08:00
type : list-single
author: Xiwei Pan
slug: knowledge-acquisition
draft: false
toc: true
categories:
  - learning
tags:
  - diffusion models
  - knowledge acquisition
---
## Preface

Diffusion models (DMs) have recently gained widespread attention due to improved sampling quality and more stable training protocols. Notable examples include [DALL·E 2](https://arxiv.org/abs/2204.06125), which generates high-quality images from text prompts, and [Sora](https://openai.com/index/video-generation-models-as-world-simulators/), which focuses on video generation. In mechanics community, [Nikolaos N. Vlassis, WaiChing Sun](https://www.sciencedirect.com/science/article/abs/pii/S0045782523002505) and [Jan-Hendrik Bastek, Dennis M. Kochmann](https://www.nature.com/articles/s42256-023-00762-x) have successfully applied these (video) denoising diffusion models to the inverse design of microstructures/metamaterials with nonlinear properties. Inspired by their work, I aim to explore these areas further, starting with DMs.

This is a collection of notes documenting my learning process on DMs. In the first few posts, I mainly focused on understanding key terminologies and the underlying mathematical and statistical principles. And I'll continue to update with code implementations when I have time. The main reference I'm using is the detailed and accessible paper [*Understanding Diffusion Models: A Unified Perspective*](https://arxiv.org/pdf/2208.11970) written by [Calvin Luo](https://www.calvinyluo.com/about.html).

## Generative Models

The goal of a **generative model** is to <font color=Crimson>learn to model the true data distribution `$p(\pmb{x})$`</font> of given observed samples `$\pmb{x}$` from a distribution of interest.

Once learned, we can <font color=Chocolate>(1) generate new samples</font> from our approximate model at will. Furthermore, under some formulations, we are able to use the learned model to <font color=Chocolate>(2) evaluate the [<font color=Chocolate>likelihood</font>](https://en.wikipedia.org/wiki/Likelihood_function) of observed or sampled data</font> as well.

### Classification

1. *Implicit generative models:* Generative Adversarial Networks (**GANs**) model the sampling procedure of a complex distribution, which is learned in an adversarial manner. (adversarial — unstable)
2. *Likelihood-based models:* It seeks to learn a model that assigns a high likelihood to the observed data samples. **Directly learn the probability density/mass function (PDF, PMF) of the distribution via (approximate) maximum likelihood.** This includes autoregressive models, normalizing flows, Variational Autoencoders (VAEs), energy-based modeling, and score-based generative models (score functions: gradients of log PDFs).
- DMs have both **likelihood-based** and **score-based** interpretations.

> Given a set of independent identically distributed data points `$\pmb{\mathrm{X}}=(x_1,\cdots,x_n)$`, where `$x_i\sim p(\pmb{x}|\pmb{\theta})$` according to some probability distribution parameterized by `$\pmb{\theta}$`, where `$\pmb{\theta}$` itself is a random variable described by a distribution, i.e. `$\pmb{\theta}\sim p(\pmb{\theta}|\pmb{\alpha})$`, the marginal likelihood in general asks what the probability `$p(\pmb{\mathrm{X}}|\pmb{\alpha})$` is, where `$\pmb{\theta}$` has been marginalized out (integrated out): `$p(\pmb{\mathrm{X}}|\pmb{\alpha})=\int_\pmb{\theta} p(\pmb{\mathrm{X}}|\pmb{\theta})\,p(\pmb{\theta}|\pmb{\alpha})\,\mathrm{d}\pmb{\theta}$`. The above definition is phrased in the context of Bayesian statistics in which case `$p(\pmb{\theta}|\pmb{\alpha})$` is called **prior density** and `$p(\pmb{\mathrm{X}}|\pmb{\theta})$` is the <font color=Red>**likelihood**</font>.

### Caveats

In generative modeling, we generally seek to learn **lower-dimensional latent representations** rather than higher-dimensional ones. This is because trying to learn a representation of higher dimension than the observation is a fruitless endeavor without strong priors. On the other hand, learning lower-dimensional latents can also be seen as a form of compression, and can potentially uncover semantically meaningful structure describing observations. A representative interpretation can be seen in the work of [Wang et al.](https://www.sciencedirect.com/science/article/abs/pii/S0045782520305624), which uses a VAE to map complex microstructures into a low-dimensional, continuous, and organized latent space.

## ELBO, VAE, and Hierarchical VAE

We can think of the data we observe as represented or generated by an associated unseen latent, random variable `$\pmb{z}$`.

### Evidence Lower BOund (ELBO)

Mathematically, the latent variables and the data we observe can be modeled by a joint distribution `$p(\pmb{x},\pmb{z})$`. The "likelihood-based" branch of generative modeling aims to learn a model to maximize the likelihood `$p(\pmb{x})$` of all observed `$\pmb{x}$`.