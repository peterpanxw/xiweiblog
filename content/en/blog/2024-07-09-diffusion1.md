---
title: "The Road to Diffusion Models: Introduction"
date: 2024-07-09T20:57:54+08:00
type : list-single
author: Xiwei Pan
slug: knowledge-acquisition
draft: false
toc: true
categories:
  - learning
tags:
  - diffusion models
  - knowledge acquisition
---
## Preface

Diffusion models (DMs) have recently gained widespread attention due to improved sampling quality and more stable training protocols. Notable examples include [DALL·E 2](https://arxiv.org/abs/2204.06125), which generates high-quality images from text prompts, and [Sora](https://openai.com/index/video-generation-models-as-world-simulators/), which focuses on video generation. In mechanics community, [Nikolaos N. Vlassis, WaiChing Sun](https://www.sciencedirect.com/science/article/abs/pii/S0045782523002505) and [Jan-Hendrik Bastek, Dennis M. Kochmann](https://www.nature.com/articles/s42256-023-00762-x) have successfully applied these (video) denoising diffusion models to the inverse design of microstructures/metamaterials with nonlinear properties. Inspired by their work, I aim to explore these areas further, starting with DMs.

This is a collection of notes documenting my learning process on DMs. In the first few posts, I mainly focused on understanding key terminologies and the underlying mathematical and statistical principles. And I'll continue to update with code implementations when I have time. The main reference I'm using is the detailed and accessible paper [*Understanding Diffusion Models: A Unified Perspective*](https://arxiv.org/pdf/2208.11970) written by [Calvin Luo](https://www.calvinyluo.com/about.html).

## Generative Models

The goal of a **generative model** is to <font color=Crimson>learn to model the true data distribution $p(x)$</font> of given observed samples $x$ from a distribution of interest.

Once learned, we can <font color=Chocolate>(1) generate new samples</font> from our approximate model at will. Furthermore, under some formulations, we are able to use the learned model to <font color=Chocolate>(2) evaluate the [likelihood](https://en.wikipedia.org/wiki/Likelihood_function) of observed or sampled data</font> as well.

### Classification

1. *Implicit generative models:* Generative Adversarial Networks (**GANs**) model the sampling procedure of a complex distribution, which is learned in an adversarial manner. (adversarial — unstable)
2. *Likelihood-based models:* It seeks to learn a model that assigns a high likelihood to the observed data samples. **Directly learn the probability density/mass function (PDF, PMF) of the distribution via (approximate) maximum likelihood.** This includes autoregressive models, normalizing flows, Variational Autoencoders (VAEs), energy-based modeling, and score-based generative models (score functions: gradients of log PDFs).
- DMs have both **likelihood-based** and **score-based** interpretations.

> Given a set of independent identically distributed data points $\mathrm{X}=(x_1,\cdots,x_n)$, where $x_i\sim p(x|\theta)$ according to some probability distribution parameterized by $\theta$, where $\theta$ itself is a random variable described by a distribution, i.e. $\theta\sim p(\theta|\alpha)$, the marginal likelihood in general asks what the probability $p(\mathrm{X}|\alpha)$ is, where $\theta$ has been marginalized out (integrated out): $p(\mathrm{X}|\alpha)=\int_\theta p(\mathrm{X}|\theta)\,p(\theta|\alpha)\,\mathrm{d}\theta$. The above definition is phrased in the context of Bayesian statistics in which case $p(\theta|\alpha)$ is called **prior density** and $p(\mathrm{X}|\theta)$ is the <font color=Red>likelihood</font>.