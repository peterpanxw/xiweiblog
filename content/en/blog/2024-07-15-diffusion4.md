---
title: "The Road to Diffusion Models: Score-based Models"
date: 2024-07-15T20:26:12+08:00
type : list-single
author: Xiwei Pan
slug: diffusion-model-score
draft: false
toc: true
categories:
  - learning
tags:
  - diffusion models
  - knowledge acquisition
---
## Score-based Generative Models
[This blog](https://xiweipan.com/en/2024/07/12/diffusion-model-vdm/) have shown that we can actually train a VDM by optimizing a neural network (NN) `$\pmb{s}_{\pmb{\theta}}(\pmb{x}_t, t)$` to predict the score function `$\nabla log\,p(\pmb{x}_t)$`. The score term in that derivation arises directly from Tweedie's Formula, which offers limited insight into the nature of the score function or the reasons for modeling it. Therefore, we resort to another class of generative models, <font color=Crimson>Score-based Generative Models</font>, for some interpretations. And from the perspective of the results, the previously derived VDM can be shown to have an equivalent Score-based Generative Modeling formulation, which allows us to flexibly switch between these two interpretations.

Instead of directly starting from showcasing "why score function?", we first revisit the **energy-based models**. Arbitrarily flexible probability distributions can be expressed by:
`$$p_{\pmb{\theta}}(\pmb{x})=\frac{1}{Z_{\pmb{\theta}}}e^{-f_{\pmb{\theta}}(\pmb{x})},$$`
where `$f_{\pmb{\theta}}(\pmb{x})$` denotes an arbitrary flexible, parameterizable function. It is called the **energy function** and often modeled by a NN. `$Z_{\pmb{\theta}}$` is a *normalizing constant* to ensure `$\int p_{\pmb{\theta}}(\pmb{x})\,\mathrm{d}\pmb{x}=1$`.