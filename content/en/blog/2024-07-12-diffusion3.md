---
title: "The Road to Diffusion Models: Variational Diffusion Models"
date: 2024-07-12T14:06:34+08:00
type : list-single
author: Xiwei Pan
slug: knowledge-acquisition
draft: false
toc: true
categories:
  - learning
tags:
  - diffusion models
  - knowledge acquisition
---
## Variational Diffusion Models (VDMs = MHVAE + 3 Restrictions)

<font color=Red>A VDM can be viewed as a *Markovian Hierarchical VAE* plus three key restrictions:</font>
> - The latent dimension is exactly equal to the data dimension
> - The structure of the latent encoder at each timestep is not learned; it is pre-defined as a linear Gaussian model. In other words, it is **a Gaussian distribution centered around the output of the previous timestep**
> - The Gaussian parameters of the latent encoders vary over time in such a way that the distribution of the latent at final timestep `$T$` is a **standard Gaussian**

The Markov property between hierarchical transitions from a standard MHVAE is explicitly maintained. We then present related explanations of the above assumptions.

**The first restriction:** with some abuse of notation, both true data sample and latent variables can be represented as `$\pmb{x}_t$`, in this way, `$t=0$` denotes true data samples and `$t\in\left[1,T\right]$` denotes a corresponding latent with hierarchy `$t$`. <font color=Chocolate>The VDM posterior</font> can be re-expressed in a form similar to the MHVAE posterior (see Equation (7) in [this Blog](https://xiweipan.com/en/2024/07/10/knowledge-acquisition/)):

`$$q(\pmb{x}_{1:T}|\pmb{x}_0)=\prod_{t=1}^Tq(\pmb{x}_t|\pmb{x}_{t-1})$$`

**The second restriction** pre-defined the distribution of each latent variable in the encoder to be a Gaussian centered around its previous hierarchical latent. Unlike a MHVAE, the structure of the encoder at each timestep is not learned, it is fixed as a linear Gaussian model, where the mean and standard deviation can be set beforehand as *hyperparameters or learned as parameters*.

We choose to parameterize the Gaussian encoder with mean `$\pmb{\mu}_t(\pmb{x}_t)=\sqrt{\alpha_t}\pmb{x}_{t-1}$`, and variance `$\sum_t(\pmb{x}_t)=(1-\alpha_t)\pmb{\mathrm{I}}$`. The coefficient forms are chosen such that the variance of the latent variables stay at a similar scale; alternatively, the encoding process is *variance-preserving*. <font color=Red>The main takeaway is that `$\alpha_t$` is a (potentially learnable) coefficient that can vary with the hierarchical depth `$t$` for flexibility.</font> The encoder transitions are mathematically denoted as:

`$$q(\pmb{x}_t|\pmb{x}_{t-1})=\mathcal{N}(\pmb{x}_t; \sqrt{\alpha_t}\pmb{x}_{t-1},(1-\alpha_t)\pmb{\mathrm{I}})$$`

**The third restriction**