---
title: "The Road to Diffusion Models: Variational Autoencoders"
date: 2024-07-10T18:20:05+08:00
type : list-single
author: Xiwei Pan
slug: knowledge-acquisition
draft: false
toc: true
categories:
  - learning
tags:
  - diffusion models
  - knowledge acquisition
---
## Variational Autoencoders (VAEs)

### Dissecting the ELBO

This approach is "*variational*" because we optimize for the best `$q_{\pmb{\phi}}(\pmb{z}|\pmb{x})$` amongst a family of potential posterior distributions parameterized by `$\pmb{\phi}$`; it is called "*autoencoder*" due to its resemblance to a traditional autoencoder model, where input data is trained to predict itself after undergoing an intermediate bottlenecking representation step. Dissecting the ELBO term:

`\begin{align}
\mathbb{E}_{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\left[log\frac{p(\pmb{x}, \pmb{z})}{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\right] &= \mathbb{E}_{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\left[log\frac{p_{\pmb{\theta}}(\pmb{x}|\pmb{z})p(\pmb{z})}{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\right] \tag{1}\\
&= \mathbb{E}_{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\left[log\,p_{\pmb{\theta}}(\pmb{x}|\pmb{z})\right]+\mathbb{E}_{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\left[log\frac{p(\pmb{z})}{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\right] \tag{2}\\
&= \underbrace{\mathbb{E}_{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\left[log\,p_{\pmb{\theta}}(\pmb{x}|\pmb{z})\right]}_{\text{reconstruction term}}-\underbrace{D_{\mathrm{KL}}\left(q_{\pmb{\phi}}(\pmb{z}|\pmb{x})\|p(\pmb{z})\right)}_{\text{prior matching term}} \tag{3} \label{eq3}
\end{align}`

From the derivation, we know that there are two main steps for the learning process:
1. Learn an intermediate bottlenecking distribution `$q_{\pmb{\phi}}(\pmb{z}|\pmb{x})$` that can be treated as an <mark>*encoder*</mark>, which transforms inputs into a distribution over possible latents.
2. Learn a deterministic function `$p_{\pmb{\theta}}(\pmb{x}|\pmb{z})$` to convert a given latent vector `$\pmb{z}$` into an observation `$\pmb{x}$`, which can be interpreted as a <mark>*decoder*</mark>.

We also present some necessary comments regarding Equation `$\eqref{eq3}$`:
- The first term measures the **reconstruction likelihood of the decoder** from our variational distribution; this ensures that the learned distribution is modeling effective latents that the original data can be regenerated from.
- The second term measures how similar the learned variational distribution is to a prior belief held over latent variables.

<font color=Red>**Maximizing the ELBO is thus equivalent to maximizing its first term and minimizing its second term.**</font>

A graphic representation can be seen in the figure below:
{{<figure src="/figures/diffusion_fig1.png" caption="Figure 1: Representation of basic encoder-decoder process in VAEs " width="220">}}

<font color=Chocolate>A defining feature of the VAE is how the ELBO is optimized jointly over parameters `$\pmb{\phi}$` and `$\pmb{\theta}$`.</font> The encoder of the VAE is generally chosen to model a **multivariate Gaussian with diagonal covariance**, and the prior is often a **standard multivariate Gaussian**:

`\begin{align}
q_{\pmb{\phi}}(\pmb{z}|\pmb{x}) &= \mathcal{N}\left(\pmb{z};\pmb{\mu}_{\pmb{\phi}}(\pmb{x}),\pmb{\sigma_{\pmb{\phi}}}^2(\pmb{x})\pmb{\mathrm{I}}\right) \tag{4}\\
p(\pmb{z}) &= \mathcal{N}\left(\pmb{z};\pmb{0},\pmb{\mathrm{I}}\right), \tag{5}
\end{align}`
with this kind of settings, the *prior matching term* of the ELBO (Equation `$\eqref{eq3}$`) can be computed analytically, and the *reconstruction term* can be approximated using a **Monte Carlo estimate**. We now rewrite our objective:

`\begin{align}
&\mathrm{arg max}\limits{\pmb{\phi}, \pmb{\theta}} \mathbb{E}_{q_{\pmb{\phi}}(\pmb{z}|\pmb{x})}\left[log\,p_{\pmb{\theta}}(\pmb{x}|\pmb{z})\right]-D_{\mathrm{KL}}\left(q_{\pmb{\phi}}(\pmb{z}|\pmb{x})\|p(\pmb{z})\right) \\
&\approx\mathrm{arg max}\limits{\pmb{\phi}, \pmb{\theta}}\sum_{l=1}^Llog\,p_{\pmb{\theta}}(\pmb{x}|\pmb{z}^{(l)})-D_{\mathrm{KL}}\left(q_{\pmb{\phi}}(\pmb{z}|\pmb{x})\|p(\pmb{z})\right), \tag{6}
\end{align}`

### Reparameterization Trick